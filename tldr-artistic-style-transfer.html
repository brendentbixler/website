<meta charset="utf-8" lang="en">
<link rel="stylesheet" type="text/css" href="style.css">
<script type="text/x-mathjax-config"> MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]} }); </script>
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/latest.js?config=TeX-MML-AM_CHTML' async></script>

<!-- Header -->

<div style="float:left;"><a href="http://apoorvaj.io/">Home</a></div>
<div style="float:right;"><a href="about.html">About</a></div>
<div style="clear:both"></div><br>
<hr style="margin: -1em 0 2em 0;"/></div>

<!-- Content -->

<title>TLDR: Artistic Style Transfer</title>
<div class="title">TLDR: Artistic Style Transfer</div>
<div class="subtitle">24-Jun-2018</div>

<center><img src="assets/style_transfer_1.jpg" style="display:inline" width="40%">
<img src="assets/style_transfer_2.jpg" style="display:inline" width="40%"></center>

<h1>Contents</h1>
<ol>
    <li><a href="#gatys">A Neural Algorithm of Artistic Style</a></li>
    <li><a href="#johnson">Perceptual Losses for Real-Time Style Transfer and Super-Resolution</a></li>
    <li><a href="#li">Universal Style Transfer via Feature Transforms (Coming soon)</a></li>
</ol>

<h1 id="gatys">1. A Neural Algorithm of Artistic Style</h1>
<h3>Convolutional layer</h3>
<p>A convolutional layer works as follows:</p>

<pre>
conv_layer(in_arr: float [IX, IY, IZ],
           filters: float [NUM_FILTERS, FLT_SZ, FLT_SZ, IZ]) -> float [NUM_FILTERS, IX, IY]
{
    let out_arr: float[IX, IY, NUM_FILTERS];
    for f in 0..NUM_FILTERS { <font color="grey">// For each filter:</font>
        let flat_filter: float[FLT_SZ * FLT_SZ * IZ] = flatten(filters[f]);
        for x in 0..IX {
            for y in 0..IY {<font color="grey"> // For each XY position in the input data:</font>
                <font color="grey">// Get neighborhood centered at (x, y) and spanning the size of the filter</font>
                let neighborhood: float[FLT_SZ, FLT_SZ, IZ] = get_filter_neighborhood(in_arr, x, y);
                let flat_in_arr: float[FLT_SZ * FLT_SZ * IZ] = flatten(neighborhood);
                let raw = dot_product(flat_in_arr, flat_filter) + bias; <font color="grey">// W*x + b</font>
                out_arr[f, x, y] = max(0, raw); <font color="grey">// ReLU activation function</font>
            }
        }
    }
    return out_arr;
}
</pre>

<table style="border-spacing: 1em 0.2em">
    <tr><td>Input</td><td>3D grid of <code>(IX * IY * IZ)</code> real numbers.</td></tr>
    <tr><td>Filters</td><td>Each filter is a 3D grid of <code>(FLT_SZ * FLT_SZ * IZ)</code> real numbers. <code>NUM_FILTERS</code> such filters. <code>FLT_SZ</code> is the filter size and is 3 in our neural network.</td></tr>
    <tr><td>Output</td><td>Grid of <code>(NUM_FILTERS * IX * IY)</code> real numbers.</td></tr>
</table>
<p>While training the network, the input and desired output are known. Filters and the bias are learned using backpropagation. During inference, input and filters are known. The output is calculated.</p>
<p>Minutiae:<br>
<code>* </code>CNNs don't perform convolutions in the strictest sense of the word; the filter is not flipped.<br>
<code>* </code>On boundary elements, the neighborhood overflows. We plug in padding values for convenience to keep input and output arrays equally sized in 2D.</p>

<h3>Max-pooling layer</h3>
<p>A max-pooling layer produces an output that is half the width, half the height and the same depth as the input. While converting a 2x2 grid of real numbers to a single real number, it outputs the maximum value among the 4 numbers it samples. Silliest downsampling ever.</p>

<h3>The VGG-19 network</h3>
<img src="assets/vgg19.svg">
<center><i><font color="grey">The convolutional layers of VGG19.<br>Max-pooling layers are not drawn, but the reduction in size comes from them.</font></i></center>
<p>VGG-19 is a convolutional network designed for image classification and trained on the ImageNet dataset. We're interested only in its convolutional and max-pooling layers. Since it's a pre-trained network, filters and biases are known. Each filter is 3x3.</p>

<h3>The core idea of the paper</h3>
<p>The content of an image can be reverse-engineered from the activations it produces in convolutional layers, and its style can be reverse-engineered from the statistical correlations of those activations. We can thus combine the content of an image with the style of another.</p>

<h3>Details</h3>
<p><ol>
<li>Pick a content image and a style image.<br><br></li>

<li>Plug in the content image as the input to the network, and observe the output $\ell_{feat}$ produced at any convolutional layer. For any input image that produces output $\ell$ at the same convolutional layer, the content loss $\mathcal{L}_{feat}$ is the Euclidean distance between flattened $\ell$ and $\ell_{feat}$.<br><br></li>

<li>Do this at all five convolutional layers, and get the five Gram matrices $G_{style}$. For any input image, we get its Gram matrices $G$. Then the style loss $\mathcal{L}_{style}$ of that image is the average of the Euclidean distances between each $G$ and its corresponding $G_{style}$.<br><br></li>

<li>Plug in a white noise image, and minimize its loss $\mathcal{L} = \lambda_{feat} \mathcal{L}_{feat} + \lambda_{style} \mathcal{L}_{style}$, where $\lambda_{feat}$ and $\lambda_{style}$ are scalar constants controlling respective contributions to the final result.</li>
</ol></p>

<h3>Pros &amp; cons</h3>
<p><code>+ </code>Works on arbitrary style images.<br>
<code>- </code>Is a training problem, since it involves backprop. Training is a much slower process than inference.</p>
<h3>Observations</h3>
<p><code>* </code>The VGG network works for this task despite being trained for image classification, not style transfer.<br>
<code>* </code>A covariance matrix works just as well as a Gram matrix for capturing style.<br>
<code>* </code>They key takeaway from the paper is not the specific technique or loss function, but it is the idea that correlations of activations in a pretrained convolutional network are a viable way of identifying artistic style.<br>
</p>

<h3>Further reading</h3>
<p><code>[1] </code><a href="https://arxiv.org/abs/1508.06576">Original paper</a> by Leon A. Gatys, Alexander S. Ecker, Matthias Bethge. <i>2015</i><br>
<code>[2] </code><a href="https://www.youtube.com/watch?v=LxfUGhug-iQ">Lecture</a> on Convolutional Neural Networks by Andrej Karpathy. <i>2016</i><br>
<code>[3] </code><a href="https://arxiv.org/abs/1409.1556">Paper</a> introducing VGG by Karen Simonyan and Andrew Zisserman. <i>2014</i>
</p>

<h1 id="johnson">2. Perceptual Losses for Real-Time Style Transfer and Super-Resolution</h1>

<center>
<img src="assets/perceptual_arch.png">
<i><font color="grey">The two appended networks. Sourced from original paper <a href="#ref_2_1"><sup>[1]</sup></a> with permission.</font></i></center>

<h3>The core idea of the paper</h3>
<p>This paper prepends a custom-made neural network to the pretrained VGG network of paper 1. The prepended network is called the image transformation network, and the VGG is called the loss network.</p>
<p>Paper 1 was slow because it was posed as a training problem. This paper makes it a fast inference problem by pre-training the prepended network for a specific style image.</p>

<h3>Details</h3>

<p>The <b>loss network</b> is a VGG-16 network, instead of a VGG-19 from the previous paper.</p>

<p>The <b>image transform network</b> has two downsampling layers, then a bunch of residual blocks, and then two upsampling layers. The residual blocks are simple but outside the scope of this article. Instead of using max-pooling, downsampling is done using a convolution stride of 2. Similarly, upsampling is done with a stride of 0.5. This makes the convolutional layers themselves responsible for changing resolution.</p>

<ol>
<li>A style image is chosen, and the activation $\ell_{style}$ and Gram matrices $G_{style}$ are obtained from the loss network, just like the previous paper.<br><br></li>

<li>We iterate over the COCO dataset, training the image transformation network to minimize the loss $\mathcal{L}$, which just like paper 1, is a weighted sum of $\mathcal{L}_{feat}$ and $\mathcal{L}_{style}$, but with an extra regularization component. We thus train the image transform network to combine the style of one image to the content of any image.<br><br></li>

<li>Once the image transform network has been trained for a given style image, any content image just needs to pass through it only once to transfer the style. This is why this paper is faster than paper 1.<br><br></li>
</ol>

<center><img src="assets/perceptual_example.png" width=100%>
<i><font color="grey">Example output. Sourced from original paper <a href="#ref_2_1"><sup>[1]</sup></a> with permission.</font></i></center>

<h3>Pros &amp; cons</h3>
<p><code>+ </code>Is an inference problem, and hence is fast.<br>
<code>+ </code>Same technique also works for super-resolution.<br>
<code>- </code>Has a big up-front cost per style image. Thus, in production, arbitrary style images cannot be selected.</p>

<h3>Observations</h3>
<p><code>* </code>The most interesting part of this paper is its incremental improvement over paper 1. This paper shows that neural networks can learn to mimic gradient descent in some circumstances.<br>
</p>

<h3>Further reading</h3>
<p>
<code id="ref_2_1">[1] </code><a href="https://arxiv.org/abs/1603.08155">Original paper</a> by Justin Johnson, Alexandre Alahi, Fei-Fei Li. <i>2016</i><br>
</p>

<h1 id="li">3. Universal Style Transfer via Feature Transforms</h1>
(Coming soon)

<!-- Footer -->

<hr style="margin: 2em 0 5 0;"/>
<center>
<a href="mailto:apoorvaj@apoorvaj.io">Email</a> / <a href="https://twitter.com/ApoorvaJ">Twitter</a> / <a href="https://github.com/ApoorvaJ">GitHub</a> / <a href="cv.html">CV</a>
</center>
<script src="include.min.js"></script>
<script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,'script','https://www.google-analytics.com/analytics.js','ga'); ga('create', 'UA-18556434-3', 'auto'); ga('send', 'pageview'); </script>
